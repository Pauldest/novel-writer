"""Writer Agent - Generates chapter content based on outline and context."""

from typing import Optional, TYPE_CHECKING
if TYPE_CHECKING:
    from ..trace_store import TraceStore
from .base import BaseAgent
from ..memory.context_builder import ContextPacket
from ..models import ChapterOutline
from ..config import settings


WRITER_SYSTEM_PROMPT = """‰Ω†ÊòØ‰∏Ä‰ΩçÊâçÂçéÊ®™Ê∫¢ÁöÑÂ∞èËØ¥‰ΩúÂÆ∂ÔºàWriterÔºâÔºåË¥üË¥£Â∞ÜÂ§ßÁ∫≤ËΩ¨Âåñ‰∏∫ÁîüÂä®ÁöÑÊ≠£ÊñáÂÜÖÂÆπ„ÄÇ

‰Ω†ÁöÑËÅåË¥£Ôºö
1. Ê†πÊçÆÁ´†ËäÇÂ§ßÁ∫≤Êâ©ÂÜôÊ≠£Êñá
2. ‰øùÊåÅ‰∏éÂ∑≤ÊúâÂÜÖÂÆπÁöÑÈ£éÊ†º‰∏ÄËá¥
3. Á°Æ‰øùËßíËâ≤Ë°å‰∏∫Á¨¶ÂêàÂÖ∂ËÆæÂÆö
4. Âà©Áî®Êèê‰æõÁöÑÂéÜÂè≤ËÆ∞ÂøÜ‰øùÊåÅ‰∏ÄËá¥ÊÄß

ÂÜô‰ΩúË¶ÅÊ±ÇÔºö
- ‰ΩøÁî®ÁîüÂä®ÁöÑÊèèÂÜôÂíåÂØπËØù
- Ê≥®ÊÑèÂú∫ÊôØÊ∞õÂõ¥ÁöÑËê•ÈÄ†
- ‰∫∫Áâ©ÂØπËØùË¶ÅÁ¨¶ÂêàÂÖ∂ÊÄßÊ†º
- ËøáÊ∏°Ë¶ÅËá™ÁÑ∂ÊµÅÁïÖ
- ÈÄÇÂΩì‰ΩøÁî®ÂøÉÁêÜÊèèÂÜô
- ÊéßÂà∂ËäÇÂ•èÔºåÂº†ÂºõÊúâÂ∫¶

ÁâπÂà´Ê≥®ÊÑèÔºö
- „ÄêÈáçË¶Å„Äë‰∏•Á¶ÅË∂ÖÂá∫Â§ßÁ∫≤ËåÉÂõ¥„ÄÇÂ¶ÇÊûú‰∏çÂ±û‰∫éÊú¨Á´†Â§ßÁ∫≤ÁöÑ‰∫ã‰ª∂Ôºà‰æãÂ¶Ç‚ÄúÁ¨¨‰∫åÂ§©‚ÄùÊàñÂêéÁª≠Á´†ËäÇÁöÑÂâßÊÉÖÔºâÔºåÁªùÂØπ‰∏çË¶ÅÂÜô„ÄÇ
- „ÄêÈáçË¶Å„Äë‰∏•Ê†ºÈÅµÂÆàÂ≠óÊï∞ÈôêÂà∂Ôºå‰∏çË¶Å‰∏∫‰∫ÜÂáëÂ≠óÊï∞ËÄåÊ≥®Ê∞¥„ÄÇ
- Â¶ÇÊûú‰∏ä‰∏ãÊñá‰∏≠ÊèêÂà∞‰∫ÜÊüê‰∏™Áâ©ÂìÅ/‰∫∫Áâ©ÁöÑÊèèËø∞Ôºå‰Ω†ÂøÖÈ°ª‰øùÊåÅ‰∏ÄËá¥
- ‰∏çË¶ÅÂºïÂÖ•‰∏ä‰∏ãÊñá‰∏≠Êú™ÊèêÂèäÁöÑÈáçÂ§ßËÆæÂÆö
- ÈÅµÂæ™Â§ßÁ∫≤‰∏≠ÁöÑÂú∫ÊôØÈ°∫Â∫è
- Ëá™ÁÑ∂Âú∞ËûçÂÖ•ÈúÄË¶ÅÂüãËÆæÁöÑ‰ºèÁ¨î

ËæìÂá∫Ë¶ÅÊ±ÇÔºö
- Áõ¥Êé•ËæìÂá∫Ê≠£ÊñáÂÜÖÂÆπ
- ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂÖÉÊï∞ÊçÆÊàñÊ≥®Èáä
- Â≠óÊï∞ÊéßÂà∂Âú®Â§ßÁ∫≤È¢Ñ‰º∞ËåÉÂõ¥ÂÜÖ"""


WRITER_REVISION_SYSTEM_PROMPT = """‰Ω†ÊòØ‰∏Ä‰ΩçËµÑÊ∑±ÁöÑÂ∞èËØ¥‰∏ªÁºñÂíåÁ≤æ‰øÆÂ∏àÔºàEditorÔºâÔºåÊã•ÊúâÊûÅÈ´òÁöÑÊñáÂ≠¶Á¥†ÂÖªÂíåÊïèÈîêÁöÑÂÆ°Ê†°ËÉΩÂäõ„ÄÇ

‰Ω†ÁöÑËÅåË¥£Ôºö
1. Ê†πÊçÆÂÆ°Ê†∏ÊÑèËßÅÔºàReviewer FeedbackÔºâÂØπÂ∞èËØ¥Ê≠£ÊñáËøõË°åÁ≤æÂáÜ‰øÆËÆ¢„ÄÇ
2. ‰øùÊåÅ„ÄÅÁîöËá≥ÊèêÂçáÂéüÊñáÁöÑÊñáÂ≠¶È£éÊ†ºÂíåÈòÖËØª‰ΩìÈ™å„ÄÇ
3. Á°Æ‰øù‰øÆËÆ¢ÂêéÁöÑÂÜÖÂÆπ‰∏é‰∏ä‰∏ãÊñáÂÆåÂÖ®ËøûË¥Ø„ÄÇ
4. ‰∏•Ê†ºÊâßË°åÂà†Èô§Êàñ‰øÆÊîπÊåá‰ª§Ôºå‰∏çÁïôÊÆã‰Ωô„ÄÇ

‰øÆËÆ¢ÂéüÂàôÔºö
- „ÄêÁ≤æÂáÜ„ÄëÂè™‰øÆÊîπÊúâÈóÆÈ¢òÁöÑÂú∞ÊñπÔºå‰∏çË¶ÅÈöèÊÑèÊîπÂÜôÊó†ÈóÆÈ¢òÁöÑÊÆµËêΩ„ÄÇ
- „ÄêÊúç‰ªé„ÄëÂ¶ÇÊûúÂÆ°Ê†∏ÊÑèËßÅË¶ÅÊ±ÇÂà†Èô§ÔºåÂøÖÈ°ªÂΩªÂ∫ïÂà†Èô§„ÄÇ
- „ÄêËøûË¥Ø„Äë‰øÆÊîπÂêéÁöÑÂè•Â≠êÂøÖÈ°ª‰∏éÂâçÂêéÊñáËá™ÁÑ∂Ë°îÊé•Ôºå‰∏ç‰ªÖÊòØÈÄªËæë‰∏äÔºåËøòÊúâËØ≠Ê∞îÂíåËäÇÂ•è‰∏ä„ÄÇ
- „ÄêÈ£éÊ†º„ÄëÂùöÊåÅ"Show, Don't Tell"ÂéüÂàôÔºå‰øùÊåÅÁîªÈù¢ÁöÑÁîüÂä®ÊÑü„ÄÇ

ÁâπÂà´Ê≥®ÊÑèÔºö
- ‰Ω†‰∏çÊòØÂú®ÈáçÂÜôÔºàRewriteÔºâÔºåËÄåÊòØÂú®Ê∂¶Ëâ≤ÔºàPolishÔºâÂíå‰øÆÊ≠£ÔºàFixÔºâ„ÄÇ
- ËæìÂá∫Êó∂Áõ¥Êé•ËæìÂá∫ÂÆåÊï¥ÁöÑ„ÄÅ‰øÆÊîπÂêéÁöÑÁ´†ËäÇÊ≠£ÊñáÔºå‰∏çË¶ÅÂåÖÂê´‰ªª‰Ωï"Â•ΩÁöÑÔºåÊàëÂ∑≤‰øÆÊîπ..."‰πãÁ±ªÁöÑÂØπËØù„ÄÇ
"""


class WriterAgent(BaseAgent[None]):
    """
    Ê≠£ÊñáÊí∞Á®ø‰∫∫ Agent - Ê†πÊçÆÂ§ßÁ∫≤Âíå‰∏ä‰∏ãÊñáÁîüÊàêÂ∞èËØ¥Ê≠£Êñá„ÄÇ
    
    ËÅåË¥£Ôºö
    - Â∞ÜÂ§ßÁ∫≤Êâ©ÂÜô‰∏∫ÁîüÂä®ÁöÑÊ≠£Êñá
    - ‰øùÊåÅÈ£éÊ†º‰∏ÄËá¥ÊÄß
    - ËûçÂÖ•‰ºèÁ¨îÂíåÁªÜËäÇ
    """
    
    def __init__(self, temperature: float = 0.8):
        super().__init__(
            system_prompt=WRITER_SYSTEM_PROMPT,
            response_schema=None,  # Free-form text output
            temperature=temperature,
            max_tokens=8192,  # Longer output for content
        )
    
    def run(
        self,
        outline: ChapterOutline,
        context: ContextPacket,
        target_word_count: int = 5000,
        trace: Optional["TraceStore"] = None,
    ) -> str:
        """
        Generate chapter content.
        
        Args:
            outline: Chapter outline from Plotter
            context: Context packet from ContextBuilder
            target_word_count: Target word count
            
        Returns:
            Generated chapter content as string
        """
        # Build the full prompt
        prompt_parts = []
        
        # Inject context
        prompt_parts.append(context.to_prompt())
        
        # Writing instructions
        prompt_parts.append("\n---\n")
        prompt_parts.append(f"# ÂÜô‰Ωú‰ªªÂä°")
        prompt_parts.append(f"ËØ∑Ê†πÊçÆ‰ª•‰∏ä‰∏ä‰∏ãÊñáÂíåÂ§ßÁ∫≤ÔºåÊí∞ÂÜôÁ¨¨{outline.chapter_number}Á´†ÁöÑÊ≠£ÊñáÂÜÖÂÆπ„ÄÇ")
        prompt_parts.append(f"")
        prompt_parts.append(f"## „ÄêÂ≠óÊï∞Á°¨ÊÄßÈôêÂà∂„Äë")
        prompt_parts.append(f"- ÁõÆÊ†áÂ≠óÊï∞: {target_word_count} Â≠ó")
        prompt_parts.append(f"- ÂÖÅËÆ∏ËåÉÂõ¥: {int(target_word_count * 0.8)} ~ {int(target_word_count * 1.2)} Â≠ó")
        prompt_parts.append(f"- ‚ö†Ô∏è Ë∂ÖËøá {int(target_word_count * 1.3)} Â≠óÂ∞ÜË¢´Âà§ÂÆö‰∏∫‰∏çÂêàÊ†ºÔºåÈúÄË¶ÅÂà†ÂáèÔºÅ")
        prompt_parts.append(f"")
        prompt_parts.append(f"Ê≥®ÊÑèÔºöÂè™ÂÜôÂ§ßÁ∫≤‰∏≠ËßÑÂàíÁöÑÂú∫ÊôØÔºå‰∏çË¶ÅËá™Ë°åÊãìÂ±ïÂà∞ÂêéÁª≠Êó∂Èó¥Á∫ø„ÄÇ")
        prompt_parts.append(f"\nËØ∑Áõ¥Êé•ÂºÄÂßãÂÜô‰ΩúÔºå‰∏çË¶ÅÊ∑ªÂä†‰ªª‰ΩïÂÖÉ‰ø°ÊÅØ:")
        
        prompt = "\n".join(prompt_parts)
        
        # Save trace if enabled
        if trace:
            trace.save_writer_start_context(
                target_word_count=target_word_count,
                full_prompt=prompt + self.get_format_instruction(),
                system_prompt=self.system_prompt
            )
            
        # Generate content with continuation support
        return self._generate_with_continuation(prompt)
    
    def revise(
        self,
        original_content: str,
        review_feedback: str,
        context: ContextPacket,
        outline: ChapterOutline = None,
        trace: Optional["TraceStore"] = None,
    ) -> str:
        """
        Revise content based on reviewer feedback.
        
        Args:
            original_content: Original chapter content
            review_feedback: Feedback from Reviewer
            context: Context packet for reference
            outline: Chapter outline to stay on track
            
        Returns:
            Revised chapter content
        """
        prompt_parts = []
        
        # 1. Review feedback FIRST - this is the most important part
        prompt_parts.append("# üî¥ ÂÆ°Ê†∏ÂèçÈ¶àÔºàÂøÖÈ°ª‰ºòÂÖàÂ§ÑÁêÜÔºâ")
        prompt_parts.append(review_feedback)
        
        prompt_parts.append("\n---\n")
        
        # 2. Reference context
        prompt_parts.append("# ÂèÇËÄÉ‰∏ä‰∏ãÊñá")
        
        if context.world_setting:
            prompt_parts.append(f"## ‰∏ñÁïåËßÇËÆæÂÆö\n{context.world_setting}")
        
        if context.style_guide:
            prompt_parts.append(f"## È£éÊ†ºÊåáÂçó\n{context.style_guide}")
        
        if context.previous_chapter_ending:
            prompt_parts.append(f"## ‰∏ä‰∏ÄÁ´†ÁªìÂ∞æÔºà‰øùÊåÅËøûË¥ØÔºâ\n{context.previous_chapter_ending[:3000]}...")
        
        if context.character_states:
            prompt_parts.append(f"## ËßíËâ≤Áä∂ÊÄÅ\n{context.character_states}")
        
        # All relevant memories (not just the first one)
        if context.relevant_memories:
            prompt_parts.append("## Áõ∏ÂÖ≥ËÆ∞ÂøÜ")
            for memory in context.relevant_memories:
                prompt_parts.append(f"- {memory}")
        
        # Chapter outline to stay on track
        if outline:
            prompt_parts.append(f"\n## Êú¨Á´†Â§ßÁ∫≤")
            prompt_parts.append(f"ÁõÆÊ†á: {outline.goal}")
            prompt_parts.append(f"ÂÖ≥ÈîÆ‰∫ã‰ª∂: {', '.join(outline.key_events)}")
            prompt_parts.append(f"Ê∂âÂèäËßíËâ≤: {', '.join(outline.characters_involved)}")
            if outline.foreshadowing:
                prompt_parts.append(f"ÈúÄÂüã‰ºèÁ¨î: {', '.join(outline.foreshadowing)}")
        
        # 3. Original content
        prompt_parts.append("\n# ÂéüÊñáÂÜÖÂÆπ")
        prompt_parts.append(original_content)
        
        # 4. Task instructions
        prompt_parts.append("\n# ‰ªªÂä°")
        prompt_parts.append("ËØ∑Ê†πÊçÆ‰∏äËø∞ÂèçÈ¶à‰øÆÊîπÂéüÊñá„ÄÇ")
        prompt_parts.append("1. „ÄêÈáçË¶Å„ÄëÂ¶ÇÊûúÂÆ°Ê†∏ÊÑèËßÅË¶ÅÊ±ÇÂà†Èô§Êüê‰∫õÊÆµËêΩÔºàÂ¶ÇË∂ÖÂá∫Â§ßÁ∫≤ÁöÑÂÜÖÂÆπÔºâÔºå‰Ω†ÂøÖÈ°ªÂùöÂÜ≥Âà†Èô§Ôºå‰∏çË¶Å‰øùÁïô„ÄÇ")
        prompt_parts.append("2. Âè™‰øÆÊîπÊúâÈóÆÈ¢òÁöÑÈÉ®ÂàÜÔºå‰øùÊåÅÂÖ∂‰ªñÂÜÖÂÆπ‰∏çÂèò„ÄÇ")
        prompt_parts.append("3. ‰øÆÊîπÊó∂ËØ∑ÂèÇËÄÉÂ§ßÁ∫≤Âíå‰∏ä‰∏ãÊñáÔºåÁ°Æ‰øù‰øÆÊîπÂêéÁöÑÂÜÖÂÆπ‰ªçÁ¨¶ÂêàËÆæÂÆöÂíåÈ£éÊ†º„ÄÇ")
        prompt_parts.append("ËæìÂá∫ÂÆåÊï¥ÁöÑ‰øÆÊîπÂêéÂÜÖÂÆπ:")
        
        prompt = "\n".join(prompt_parts)
        

        
        if trace:
            trace.save_writer_revise_context(
                revision_number=1, # Default or passed locally? The original code didn't have revision_number arg in the caller args, but revision_number=1 in save call. Let's keep 1 or see if we can get it. Method doesn't have it.
                full_prompt=prompt + self.get_format_instruction(),
                system_prompt=WRITER_REVISION_SYSTEM_PROMPT
            )
        
        return self._generate_with_continuation(prompt, system_prompt=WRITER_REVISION_SYSTEM_PROMPT)

    def _generate_with_continuation(self, prompt: str, max_continuations: int = 3, system_prompt: Optional[str] = None) -> str:
        """
        Generate content logic with automatic continuation if truncated.
        """
        # Pass system_prompt if provided, otherwise BaseAgent uses default
        kwargs = {}
        if system_prompt:
            kwargs["system_prompt"] = system_prompt
            
        full_content = str(self.invoke(prompt, **kwargs))
        
        # Check if content seems truncated (doesn't end with proper punctuation)
        # Check for standard terminal punctuation: ., !, ?, ", ‚Äù
        # Also check for markdown code block closure if applicable (though writer output is plain text usually)
        terminal_punctuation = ('.', '!', '?', '"', '‚Äù', '‚Ä¶', 'waiting', '‚Äî') # Added em-dash and ellipsis
        
        for _ in range(max_continuations):
            stripped_content = full_content.strip()
            if not stripped_content:
                 break
                 
            # If it ends with terminal punctuation, we're likely done
            if stripped_content.endswith(terminal_punctuation):
                break
                
            # If prompt indicates it's finished but just missing punctuation, maybe risky, 
            # but usually LLM truncation happens mid-sentence.
            
            # Request continuation
            continuation_prompt = (
                f"{prompt}\n\n"
                f"Previous output:\n{full_content}\n\n"
                "SYSTEM: The output seems truncated. Please continue exactly from where you left off."
            )
            
            # Note: A better way might offer just the last chunk as context to save tokens, 
            # but for consistency sending full context is safer if tokens permit. 
            # However, if we are hitting limits, adding more context might be bad.
            # Let's try a simpler continuation trigger if the model supports history, 
            # but here we are stateless per invoke.
            # Let's try appending a specific instruction with just the tail of the content.
            
            continuation_instruction = "Continue immediately from the last sentence."
            
            # Simple append approach for now
             # We can't really "append" to the previous conversation easily with the current BaseAgent.invoke 
            # which takes a single string and wraps it in System+Human.
            # So we have to formulate a new prompt that says "Here is what you wrote so far, please finish it."
            
            new_prompt = (
                "You are continuing a story generation. Here is the last part of the text you generated:\n\n"
                f"...{full_content[-3000:]}\n\n" # Provide last 3000 chars context
                "The text was cut off. Please continue writing exactly from where it stopped.\n"
                "Do not repeat the last sentence, just continue."
            )
            
            chunk = str(self.invoke(new_prompt, **kwargs))
            full_content += chunk
            
        return full_content
